{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collecting the relevant functions for processing files in a single stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Processing, No Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import roman\n",
    "import os\n",
    "#now let's get all the baseline Treatise Reference Data\n",
    "from treatise_reference_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expand a range into a list\n",
    "def rangeExpander(cite_in):\n",
    "    page_range = []\n",
    "    dash_expression = re.compile('[-–—]')\n",
    "    if dash_expression.search(cite_in):\n",
    "        first_num = re.split(dash_expression, cite_in)[0]\n",
    "        second_num = re.split(dash_expression, cite_in)[1]\n",
    "        diff_in_len = len(first_num)-len(second_num)\n",
    "        if diff_in_len >= 0:\n",
    "            expanded_second_num = first_num[:diff_in_len]+second_num\n",
    "            for num in range(int(first_num), int(expanded_second_num)+1):\n",
    "                page_range.append(str(num))\n",
    "        elif diff_in_len < 0:\n",
    "            for num in range(int(first_num), int(second_num)+1):\n",
    "                page_range.append(str(num))\n",
    "    else:\n",
    "        page_range.append(cite_in)\n",
    "    return page_range\n",
    "\n",
    "#Expand a list into a collection\n",
    "def listExpander(cite_in):\n",
    "    page_list = []\n",
    "    if ',' in cite_in:\n",
    "        for entry in cite_in.split(','):\n",
    "            page_list.append(entry.strip())\n",
    "    else:\n",
    "        page_list.append(cite_in)\n",
    "    return page_list\n",
    "\n",
    "#Expand lists into collections of individuals and ranges and then expand the ranges into collections\n",
    "def fullExpander(cite_in):\n",
    "    final_list = []\n",
    "    for entry in listExpander(cite_in):\n",
    "        for item in rangeExpander(entry):\n",
    "            final_list.append(item)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dealing with Roman Numerals\n",
    "#First, convert roman numerals to arabic for individual pages, and if it's a range, convert each side of the range from roman to arabic\n",
    "def romanDigitRangeCorrector(cite_in):\n",
    "    dash_expression = re.compile('[-–—]')\n",
    "    if dash_expression.search(cite_in):\n",
    "\n",
    "        first_roman_num = re.split(dash_expression, cite_in)[0]\n",
    "        second_roman_num = second_num = re.split(dash_expression, cite_in)[1]\n",
    "        first_num = str(roman.fromRoman(first_roman_num.upper()))\n",
    "        second_num = str(roman.fromRoman(second_roman_num.upper()))\n",
    "        arabic_range = first_num + '-' + second_num\n",
    "\n",
    "        return arabic_range\n",
    "    else:\n",
    "        return str(roman.fromRoman(cite_in.upper()))\n",
    "    \n",
    "#Second process roman page ranges and lists into collections of roman page numbered ranges and lists\n",
    "def romanHandler(cite_in):\n",
    "    final_out = []\n",
    "    roman_in_list = listExpander(cite_in)\n",
    "    arabic_intermediate_list = []\n",
    "    for entry in roman_in_list:\n",
    "        arabic_intermediate_list.append(romanDigitRangeCorrector(entry))\n",
    "    arabic_final_list = []\n",
    "    for entry in arabic_intermediate_list:\n",
    "        for page in fullExpander(entry):\n",
    "            arabic_final_list.append(page)\n",
    "    for page in arabic_final_list:\n",
    "        final_out.append(roman.toRoman(int(page)).lower())\n",
    "    \n",
    "    return final_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Function for getting a list of Pages/Paragraphs in the citation no matter roman or not:\n",
    "#NOTE THAT THIS FUNCTIONS AS SOMETHING LIKE THE SBN CLEANER\n",
    "def getListOfPagesFromCite(cite_in):\n",
    "    final_out = []\n",
    "    list_version_of_cite = listExpander(cite_in)\n",
    "    roman_check = re.compile('[xvi]|[XVI]')\n",
    "    for cite in list_version_of_cite:\n",
    "        if roman_check.search(cite):\n",
    "            for page in romanHandler(cite):\n",
    "                final_out.append(page)\n",
    "        else:\n",
    "            for page in fullExpander(cite):\n",
    "                final_out.append(page)\n",
    "\n",
    "    return final_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The foregoing functinos work for individual numbers, whether representing pages, paragraphs, sections, not for citations that are in norton style. Instead the above functions can be applied to a parsed Norton citation Below we'll work with norton citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first check if we're dealing with Roman numerals and replace with arabic\n",
    "def romanSubber(text):\n",
    "    roman_checker = re.compile('[ixv]|[IXV]')\n",
    "    if roman_checker.search(text):\n",
    "        return str(roman.fromRoman(text.upper()))\n",
    "    else:\n",
    "        return text\n",
    "#second, check if we're dealing with any odd dashes like EM or EN dashes instead of plain dashes and replace with regular dash.\n",
    "def dashFixer(text):\n",
    "    dash_checker = dash_checker = re.compile('[-–—]')\n",
    "    if dash_checker.search(text):\n",
    "        dash_list = re.split(dash_checker, text)\n",
    "        new_dash_list = []\n",
    "        for dash_item in dash_list:\n",
    "            new_dash_list.append(romanSubber(dash_item))\n",
    "        text_out = '-'.join(new_dash_list)\n",
    "        return text_out\n",
    "    else:\n",
    "        return text\n",
    "#third, we're going to check inside lists and replace romans and dashes that occur insdie comma separated lists.\n",
    "def commaFixer(text):\n",
    "    if ',' in text:\n",
    "        new_comma_list = []\n",
    "        for entry in text.split(','):\n",
    "            new_entry = dashFixer(entry)\n",
    "            new_entry = romanSubber(new_entry)\n",
    "            new_comma_list.append(new_entry)\n",
    "\n",
    "        return ','.join(new_comma_list)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "#fourth, we're going to break a norton citation down by the '.'s used to delineate the structure and fix any weird dashes or commas.\n",
    "def romanCleaner(cite_in):\n",
    "    cite_out_list = []\n",
    "    for item in cite_in.split('.'):\n",
    "        to_write = item\n",
    "        to_write = commaFixer(to_write)\n",
    "        to_write = dashFixer(to_write)\n",
    "        to_write = romanSubber(to_write)\n",
    "\n",
    "        cite_out_list.append(to_write)\n",
    "\n",
    "    cite_out = '.'.join(cite_out_list)\n",
    "\n",
    "    return cite_out\n",
    "\n",
    "#fifth, we're going to make our Appendix references uniform as \"App.\"\n",
    "def dotAppAdder(cite_in):\n",
    "    appChecker = re.compile('(App(?![.|e]))|(Appendix)')\n",
    "    if appChecker.search(cite_in):\n",
    "        return re.sub(appChecker, 'App.', cite_in)\n",
    "    else:\n",
    "        return cite_in\n",
    "\n",
    "#sixth, we're going to make our references to teh Abstract uniform as 'Abs' from Abstract or Abs.\n",
    "def abstractCleaner(cite_in):\n",
    "    absChecker = re.compile('(Abs\\.)|(Abstract)')\n",
    "    if absChecker.search(cite_in):\n",
    "        return re.sub(absChecker, 'Abs', cite_in)\n",
    "    else:\n",
    "        cite_out = cite_in\n",
    "    return cite_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pull all of the above into a single function that cleans norton citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullNortonCleaner(cite_in):\n",
    "    cite_out = abstractCleaner(cite_in)\n",
    "    cite_out = dotAppAdder(cite_out)\n",
    "    if cite_out[0] != \"A\":\n",
    "        cite_out = romanCleaner(cite_out)\n",
    "    return cite_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not quite done yet parsing Norton citations to get them ready for scoring yet, however. The norton cleaner just gets us back a citation that's been made uniform but not parsed into a list of paragraphs yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we need to expand a list or range of paragraphs that are given.\n",
    "def nortonExpander(cite_in):\n",
    "    final_out = []\n",
    "    #first, establish the chapter\n",
    "    capture_chapter = re.compile('(\\d+\\.\\d+\\.\\d+)|(App)|(Abs)')\n",
    "    #second, create the chapter slice:\n",
    "    chapter_slice = cite_in[:capture_chapter.match(cite_in).end()]\n",
    "    #next, create the remainder slice.\n",
    "    remainder_slice = cite_in[capture_chapter.match(cite_in).end():]\n",
    "    #if the chapter is all there is, we'll just append the chapter to the list out\n",
    "    if remainder_slice == '':\n",
    "        final_out.append(cite_in)\n",
    "\n",
    "    #if the remainder slice isn't empty and it isn't a ., suggesting there's a paragraph\n",
    "    #ie if it's going to be a range or list of chapters\n",
    "    elif remainder_slice[0] != '':\n",
    "        if remainder_slice[0] !='.':\n",
    "            #we're going to find the last match of '.numbers'\n",
    "            #the 's' is in the re to capture abstract cases, App. is already captured by dot\n",
    "            dot_d_finder = re.compile('[.s]\\d+')\n",
    "            d_list = dot_d_finder.finditer(cite_in)\n",
    "            for match in d_list:\n",
    "                last_match = match\n",
    "            #we'll take the start position of that, ie the last dot, and add one\n",
    "            chapter_start = last_match.start() + 1\n",
    "            chapter_range = cite_in[chapter_start:]\n",
    "            #now let's run it through a listExpander and then the range expander:\n",
    "            for list_entry in listExpander(chapter_range):\n",
    "                for range_entry in getListOfPagesFromCite(list_entry):\n",
    "                    final_out.append(cite_in[:chapter_start]+range_entry)\n",
    "\n",
    "    #if the cite is neither a solitary chapter nor a list or range of chapters, then it's a paragraph\n",
    "    #or list or range of paragraphs, so we'll epxand that.\n",
    "        else:\n",
    "            #create a list of everything that remains, split by ,\n",
    "            remainder_list = listExpander(remainder_slice)\n",
    "            for para in remainder_list:\n",
    "                #remove the . from any item in that list\n",
    "                new_para = para.replace('.','')\n",
    "                #expand the ranges of anything and then append it\n",
    "                for item in getListOfPagesFromCite(new_para):\n",
    "                    final_out.append(chapter_slice+'.'+item)\n",
    "\n",
    "    return final_out\n",
    "\n",
    "#Second, if a norton cite is to a chapter, we need to expand that into the list of paragraphs\n",
    "#in the chapter to get it ready for scoring.\n",
    "def nortonChapterExpander(cite_in):\n",
    "    final_list = []\n",
    "    is_a_chapter = re.compile('(\\d+\\.\\d+\\.\\d+)(?!\\.)')\n",
    "    if is_a_chapter.match(cite_in) != None:\n",
    "        for para in treatise_paragraph_list:\n",
    "            if cite_in == para[:len(cite_in)]:\n",
    "                final_list.append(para)\n",
    "        if len(final_list)<1:\n",
    "            final_list.append(cite_in)\n",
    "    else:\n",
    "        final_list.append(cite_in)\n",
    "    return final_list\n",
    "\n",
    "#Third, finally, we can pull these two together to get the full\n",
    "#expansion of any citation into a list of paragraphs:\n",
    "def nortonFullExpander(cite_in):\n",
    "    final_list = []\n",
    "    for entry in nortonExpander(cite_in):\n",
    "        for item in nortonChapterExpander(entry):\n",
    "            final_list.append(item)\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have two fully capable parsers to get a list of paragraphs or pages we can start moving towards scoring. Scoring assigns points to the paragraphs in the Treatise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First let's look at SBN Scoring. This function will take an SBN \n",
    "#citation in and return a list of paragraphs and the proportion of\n",
    "#the citation given over to that paragraph\n",
    "def sbnScoring(cite_in):\n",
    "    #get all the pages from the citation in a list\n",
    "    page_list = getListOfPagesFromCite(cite_in)\n",
    "    #get the count of pages, to be used in distributing credit from the citation across all pages\n",
    "    total_pages = len(page_list)\n",
    "    #this number will be used to noramlize our weights back up to 1 by dividing 1 by it\n",
    "    total_para_weights = 0\n",
    "\n",
    "    #now we'll get a list of all the paras, paired with their page-relative-weight\n",
    "    pre_normalized_list_of_para_weight_pairs = []\n",
    "    for page in page_list:\n",
    "        try:\n",
    "            para_list = sbn_to_norton_dictionary[page]\n",
    "            total_paras = len(para_list)\n",
    "            for para in para_list:\n",
    "                pre_normalized_list_of_para_weight_pairs.append((para, 1/total_paras))\n",
    "                total_para_weights += 1/total_paras\n",
    "        except KeyError:\n",
    "            print(page, \"generated a key error when trying to find matching paragraphs\")\n",
    "\n",
    "    #now we'll normalize all page-relative para weights so they add up to 1\n",
    "    final_list_of_para_weight_pairs = []\n",
    "    normalizing_factor = 1/total_para_weights\n",
    "    total_weight_check = 0\n",
    "    for pair in pre_normalized_list_of_para_weight_pairs:\n",
    "        try:\n",
    "            final_list_of_para_weight_pairs.append((pair[0], round(pair[1]*normalizing_factor,3)))\n",
    "            total_weight_check += pair[1]*normalizing_factor\n",
    "        except KeyError:\n",
    "               print(page, \"generated a Key Error when trying to find correspondign paragraphs\")\n",
    "    #print(total_weight_check)\n",
    "    return final_list_of_para_weight_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second, we can take norton scoring which is a bit simpler because we don't have to \n",
    "#convert between pages and paragraphs.\n",
    "\n",
    "def nortonScoring(cite_in):\n",
    "    final_list = []\n",
    "    total_paras = len(nortonFullExpander(cite_in))\n",
    "    for para in nortonFullExpander(cite_in):\n",
    "        final_list.append((para, round(1/total_paras,3)))\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can pull out an Ultimate Score Parser which will take in either an SBN or a Norton cite and return a list of paragraphs and weights/% of the citation that each paragraph should get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This pulls together all of the foregoing functions. It should be all that you need\n",
    "#to import from the functions along with the treatise reference information to take a citation in and \n",
    "#get a list of paragraphs and weights.\n",
    "def ultimateParser(cite_in):\n",
    "    norton_check = re.compile(\"(T?A)|([123Ii]+\\.)+\")\n",
    "    if norton_check.match(cite_in) != None:\n",
    "        #let's just make sure we don't have a 'T'\n",
    "        if cite_in[0]==\"T\":\n",
    "            clean_cite = cite_in[1:]\n",
    "        else:\n",
    "            clean_cite = cite_in\n",
    "        cleaner_cite = fullNortonCleaner(clean_cite)\n",
    "        #need to add a citation quality checker here. we have one by default\n",
    "        #with the Key Errors for SBN cases but not for Norton cases\n",
    "        for pair in nortonScoring(cleaner_cite):\n",
    "            if not pair[0] in treatise_paragraph_list:\n",
    "                print(pair[0], \"is not a paragraph in the Treatise, check the citation\")\n",
    "        return nortonScoring(cleaner_cite)\n",
    "    else:\n",
    "        return sbnScoring(cite_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think for single file processing, I won't need to pull anything from the script_to_generate_paragraph_level_citations because that works by pulling each paragraph/weight pair from the big master sheet where each citation is its own row in a spreadsheet.\n",
    "\n",
    "What I want to do, then, for a single processed file is to add each pair to a total list for the citation.\n",
    "\n",
    "article_list_of_citations and weights = []\n",
    "For each citation in article:\n",
    "    For each pair in ultimateParser(citation):\n",
    "        article_list_of_citations_and_weights.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
