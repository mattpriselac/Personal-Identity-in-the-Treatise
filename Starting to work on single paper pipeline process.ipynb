{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single file processing pipeline development.\n",
    "Stage one, take in a pdf, process it to a no-ws txt file\n",
    "--I'm going to leave this stage out for now, i'll just import a file.\n",
    "Stage two, run the search process on the overall file\n",
    "Stage three, produce a score sheet for an individual file.\n",
    "Stage four, generate a comparison with overall scholarship\n",
    "Stage five, generate similarity and difference files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'search_texts/'\n",
    "file_name = '533Cottrell.txt'\n",
    "\n",
    "file_in = open(path+file_name, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#create fresh score sheet\n",
    "import csv\n",
    "\n",
    "blank_score_dict = {}\n",
    "blank_score_sheet = open('data_out/blank_master_score_sheet.csv', 'r')\n",
    "csvreader = csv.reader(blank_score_sheet)\n",
    "for row in csvreader:\n",
    "    blank_score_dict[row[0]] = row[1]\n",
    "blank_score_sheet.close()\n",
    "del blank_score_dict['paragraph']\n",
    "print(blank_score_dict['2.2.12.4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in blank_score_dict.keys():\n",
    "    blank_score_dict[key] = int(blank_score_dict[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got our keys as paragraphs and our values as integers. Next we need to import the processing functions\n",
    "My strategy here is to just take in the pieces from the data_generating_script.py file but I don't need the dict informatino because I'm going to just create the paper object directly from the file.\n",
    "I think the main thing to do is to create the dictionaries and then only import the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Article_and_Citation_Classes import Citation, Paper\n",
    "fileToPathDict = {}\n",
    "fileToPaperDict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileToPathDict[file_name] = path+file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search_texts/533Cottrell.txt'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileToPathDict[file_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ii'm not sure why I get a key error for file_name when trying to generate the class is generating a key error here. For now I'm just going to take in abbreviated versons of the classes that don't require the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Citation:\n",
    "    def __init__(self, paper, order_num, search_result):\n",
    "        self.order = order_num\n",
    "        self.paper = fileToPaperDict[paper]\n",
    "        self.search_term = \"\"\n",
    "        self.citationScores = []\n",
    "        self.startPoint = 0\n",
    "        self.endPoint = 0\n",
    "        self.rawCitationText = search_result\n",
    "        self.precedingText = \"\"\n",
    "        self.trailingText = \"\"\n",
    "        self.cleanedCitation = \"\"\n",
    "\n",
    "    #this function pulls a given number of characters from before the citation starts up to the start\n",
    "    #of the citation\n",
    "    def FindPrecedingText(self, num_chars):\n",
    "        #open the paper file\n",
    "        paper_file = open(self.paper.path, \"r\")\n",
    "        text_to_use = paper_file.readline()\n",
    "        paper_file.close()\n",
    "        #generate the buffer to get the appropriate slice in case it's around an edge of teh string\n",
    "        buffer = self.startPoint - num_chars\n",
    "        #set the proper text in the Citation\n",
    "        if self.startPoint == 0:\n",
    "            self.precedingText = ''\n",
    "        elif buffer >= 0:\n",
    "            self.precedingText = text_to_use[buffer:self.startPoint]\n",
    "        elif buffer < 0:\n",
    "            self.precedingText[:self.startPoint]\n",
    "\n",
    "    #this function pulls a given number of characters from the end of the citation going forward\n",
    "    def FindTrailingText(self, num_chars):\n",
    "        #open the paper file\n",
    "        paper_file = open(self.paper.path, \"r\")\n",
    "        text_to_use = paper_file.readline()\n",
    "        paper_file.close()\n",
    "        #generate the buffer to get the appropriate slice in case it's around an edge of the string\n",
    "        buffer = len(text_to_use) - (self.endPoint + num_chars)\n",
    "        #set the proper text in the Citation\n",
    "        if buffer == 0:\n",
    "            self.trailingText = \"\"\n",
    "        elif buffer > 0:\n",
    "            self.trailingText = text_to_use[self.endPoint:(self.endPoint + num_chars)]\n",
    "        elif buffer < 0:\n",
    "            self.trailingText = text_to_use[self.endPoint:]\n",
    "\n",
    "    #this function takes an integer as input and runs the previous two functions\n",
    "    def PopulateSurroundingTexts(self, num_chars):\n",
    "        self.FindPrecedingText(num_chars)\n",
    "        self.FindTrailingText(num_chars)\n",
    "\n",
    "    def calculateScore(self):\n",
    "        self.citationScores = ultimateParser(self.cleanedCitation)\n",
    "class Paper:\n",
    "    def __init__(self, filename):\n",
    "        self.name = filename\n",
    "        self.path = ''\n",
    "        self.author = ''\n",
    "        self.year = ''\n",
    "        self.title = ''\n",
    "        self.nortonCites = []\n",
    "        self.sbnCites = []\n",
    "        self.rawParenthesesCapture = []\n",
    "        self.otherCites = []\n",
    "        self.scoring_output = []\n",
    "\n",
    "    def NortonSearch(self):\n",
    "        citationCounter = 0 #citation counter to be used in numbering citations\n",
    "        #create the citation search object\n",
    "        #search pattern is\n",
    "##        Capture Abstract cites with optional dash additions\n",
    "##        Capture Appendix cites with optional dash additions\n",
    "##        Capture Main body cites as follows:\n",
    "##            Book. (capture both roman and arabic numeral versions)\n",
    "##            Part. (capture both roman (capitalized and not) and arabic numerals)\n",
    "##            Section (I left of the '.' here to be able to capture citations that are only Book.Part.Section with no paragraph citation)\n",
    "##            optional .Paragraph(s with optional dash separator for a range of paragraphs\n",
    "        nortonPattern = re.compile(\"\"\"  T*Abs\\d+\n",
    "                                            ([-–—]\\d{1,2}){0,1}|\n",
    "                                        T*App\\d+\n",
    "                                            ([-–—]\\d{1,2}){0,1}|\n",
    "                                        ((I{1,3}|[123])\\.)\n",
    "                                        (([i]{1,3}|IV|[I]{1,3}|[1-4])\\.)\n",
    "                                        (\\d{1,2})\n",
    "                                        (\\.[1-9]\\d{0,2}\n",
    "                                            ([-–—]\\d{1,2}){0,1}\n",
    "                                        ){0,1}\"\"\", re.X)\n",
    "        #make the text of the paper accessible and generate the match objects\n",
    "        paper_to_search = open(self.path, \"r\")\n",
    "        text_to_search = paper_to_search.readline()\n",
    "        paper_to_search.close()\n",
    "        matchObjects = nortonPattern.finditer(text_to_search)\n",
    "        #create a citation for each match object\n",
    "        for match in matchObjects:\n",
    "            citationCounter +=1\n",
    "            citationObject = Citation(self.name, citationCounter, match.group())\n",
    "            citationObject.startPoint = match.start()\n",
    "            citationObject.endPoint = match.end()\n",
    "            citationObject.search_term = match.re\n",
    "            self.nortonCites.append(citationObject)\n",
    "        if len(self.nortonCites) == 0:\n",
    "            pass\n",
    "\n",
    "    def SbnSearch(self):\n",
    "        citationCounter = 0 #citation counter to be used in numbering citations\n",
    "        #create the citation search object\n",
    "##        the search pattern this time is to start with SBN\n",
    "##        then cover page numbers (I got rid of 0 as a starting point because a file happened to have a weird SBN0 followed by a long string of numbers\n",
    "##        next I have an optional dash and comma separator that can be repeated to capture the multiple pages and ranges that get cited\n",
    "##        this will require some cleaning because sometimes you get a random 'i' following the comma\n",
    "        sbnPattern = re.compile(\"\"\" (?<!I)\n",
    "                                    (SBN)\n",
    "                                    ([1-9]\\d+|[xvi]+|[XVI]+)\n",
    "                                    ([-–—,](\\d+|[xvi]+|[XVI]+))*\"\"\", re.X)\n",
    "        #make the text of the paper accessible and generate the match objects\n",
    "        paper_to_search = open(self.path, \"r\")\n",
    "        text_to_search = paper_to_search.readline()\n",
    "        paper_to_search.close()\n",
    "        matchObjects = sbnPattern.finditer(text_to_search)\n",
    "        #create a citation for each match object\n",
    "        for match in matchObjects:\n",
    "            citationCounter +=1\n",
    "            citationObject = Citation(self.name, citationCounter, match.group())\n",
    "            citationObject.startPoint = match.start()\n",
    "            citationObject.endPoint = match.end()\n",
    "            citationObject.search_term = match.re\n",
    "            self.sbnCites.append(citationObject)\n",
    "        if len(self.sbnCites) == 0:\n",
    "            pass\n",
    "\n",
    "    def parenthesesCapture(self):\n",
    "        citationCounter = 0 #citation counter to be used in numbering citations\n",
    "        #create the citation search object\n",
    "        #This idea behind this search string is to get anything in parentheses with the following structure:\n",
    "            #First, it can optionally start with either a 'T', 'THN', 'Treatise', or 'Hume'\n",
    "            #Second, there can be a run of some intervening text but not a close parens or any numbers\n",
    "            #Third, we get a page number citation with an optional p, p., or pp.\n",
    "            #fourth, we get up to a three digit page number or range of up to 3 digit page numbers in either\n",
    "                #roman or arabic numerals\n",
    "            #The only thing I haven't figured out how to capture yet is a brief comment that appears in a few\n",
    "            #cases where the authro says something like, 'my emphases' or 'italics mine'. I think that might\n",
    "            #require a different search with a more restrictive start to the parentheses\n",
    "        pattern = re.compile('\\((T|THN|Treatise|Hume)*([A-Z]|[a-z]|[,.])*(p*\\.{0,1}(\\d{1,3}|[xvi]+|[XVI]+)([-–—,](\\d+|[xvi]{1,5}|[XVI]{1,5}))*)\\)')\n",
    "        #make the text of the paper accessible and generate the match objects\n",
    "        paper_to_search = open(self.path, \"r\")\n",
    "        text_to_search = paper_to_search.readline()\n",
    "        paper_to_search.close()\n",
    "        matchObjects = pattern.finditer(text_to_search)\n",
    "        #create a citation for each match object\n",
    "        for match in matchObjects:\n",
    "            citationCounter +=1\n",
    "            citationObject = Citation(self.name, citationCounter, match.group())\n",
    "            citationObject.startPoint = match.start()\n",
    "            citationObject.endPoint = match.end()\n",
    "            citationObject.search_term = match.re\n",
    "            self.rawParenthesesCapture.append(citationObject)\n",
    "        if len(self.rawParenthesesCapture) == 0:\n",
    "            pass\n",
    "\n",
    "    #this method takes a search string and will return a list of citation objects that match the string.\n",
    "    def otherSearch(self, search_term):\n",
    "        citationCounter = 0 #citation counter to be used in numbering citations\n",
    "        #create the citation search object\n",
    "        pattern = re.compile(search_term)\n",
    "        #make the text of the paper accessible and generate the match objects\n",
    "        paper_to_search = open(self.path, \"r\")\n",
    "        text_to_search = paper_to_search.readline()\n",
    "        paper_to_search.close()\n",
    "        matchObjects = pattern.finditer(text_to_search)\n",
    "        #create a citation for each match object\n",
    "        for match in matchObjects:\n",
    "            citationCounter +=1\n",
    "            citationObject = Citation(self.name, citationCounter, match.group())\n",
    "            citationObject.startPoint = match.start()\n",
    "            citationObject.endPoint = match.end()\n",
    "            citationObject.search_term = match.re\n",
    "            self.otherCites.append(citationObject)\n",
    "        if len(self.otherCites) == 0:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_obj = Paper(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileToPaperDict[paper_obj.name]=paper_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_obj.path = path+file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search_texts/533Cottrell.txt'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_obj.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "paper_obj.nortonCites = []\n",
    "paper_obj.NortonSearch()\n",
    "print(len(paper_obj.nortonCites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from master_function_list import * \n",
    "from treatise_reference_data import *\n",
    "for cite in paper_obj.nortonCites:\n",
    "    cite.cleanedCitation = cite.rawCitationText\n",
    "    cite.calculateScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('App.1', 1.0)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_obj.nortonCites[5].citationScores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting a bit messy, I need to go back and think through the pipeline process here under the constraints of not having the whole set of papers to work with but just having to work with one. That might inform a revision of the big project generation of data as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
